{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3, 1, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 9, 3, 1, 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 9, 120)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.relu1(self.conv1(x)))\n",
    "        out = self.pool2(self.relu2(self.conv2(out)))\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.relu3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1528],\n",
      "          [0.0000, 0.0492, 0.0000,  ..., 0.0025, 0.1912, 0.2341],\n",
      "          [0.0000, 0.0837, 0.0000,  ..., 0.0000, 0.0000, 0.2475],\n",
      "          ...,\n",
      "          [0.0000, 0.0264, 0.0000,  ..., 0.0000, 0.0000, 0.1862],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0852, 0.0630, 0.0806],\n",
      "          [0.0000, 0.1087, 0.0187,  ..., 0.0000, 0.0000, 0.1149]],\n",
      "\n",
      "         [[0.0000, 0.0186, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.1198, 0.0664, 0.0518],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0178, 0.0171, 0.0000,  ..., 0.0789, 0.0000, 0.0000],\n",
      "          [0.1214, 0.2143, 0.2529,  ..., 0.2265, 0.1732, 0.3141]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0963,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.3041, 0.3204, 0.4684,  ..., 0.4092, 0.3068, 0.3785],\n",
      "          [0.4291, 0.1315, 0.4122,  ..., 0.1868, 0.0574, 0.1417],\n",
      "          [0.3971, 0.2948, 0.2059,  ..., 0.2533, 0.1920, 0.1493],\n",
      "          ...,\n",
      "          [0.3071, 0.1369, 0.2201,  ..., 0.3947, 0.2202, 0.0000],\n",
      "          [0.4310, 0.0761, 0.3596,  ..., 0.2266, 0.3063, 0.3765],\n",
      "          [0.3277, 0.3708, 0.3710,  ..., 0.3760, 0.4216, 0.4656]],\n",
      "\n",
      "         [[0.5748, 0.6403, 0.6851,  ..., 0.6582, 0.7116, 0.7351],\n",
      "          [0.6401, 0.5146, 0.7690,  ..., 0.6665, 0.4772, 0.4952],\n",
      "          [0.6066, 0.7120, 0.6673,  ..., 0.7037, 0.8548, 0.7497],\n",
      "          ...,\n",
      "          [0.6695, 0.7396, 0.7124,  ..., 0.5545, 0.7734, 0.5452],\n",
      "          [0.7064, 0.7472, 0.7069,  ..., 0.8151, 0.8526, 0.6032],\n",
      "          [0.6374, 0.5600, 0.7253,  ..., 0.7088, 0.6985, 0.6190]]]],\n",
      "       grad_fn=<MaxPool2DWithIndicesBackward0>),) tensor([[[[ 0.0197,  0.2010,  0.1372,  ...,  0.1804,  0.2226,  0.0801],\n",
      "          [ 0.0528,  0.2233,  0.2036,  ...,  0.2079,  0.2092,  0.0652],\n",
      "          [ 0.0499,  0.1748,  0.2231,  ...,  0.1395,  0.1900,  0.0635],\n",
      "          ...,\n",
      "          [ 0.0521,  0.2448,  0.2060,  ...,  0.1924,  0.1054,  0.0331],\n",
      "          [ 0.0734,  0.2259,  0.1589,  ...,  0.2542,  0.2087,  0.0314],\n",
      "          [ 0.1001,  0.1507,  0.1689,  ...,  0.1966,  0.1577, -0.0147]],\n",
      "\n",
      "         [[-0.1397, -0.2011, -0.2459,  ..., -0.2505, -0.2852, -0.2551],\n",
      "          [-0.1506, -0.2242, -0.2044,  ..., -0.2699, -0.2220, -0.2852],\n",
      "          [-0.1837, -0.1884, -0.2618,  ..., -0.2512, -0.2169, -0.2763],\n",
      "          ...,\n",
      "          [-0.1902, -0.2169, -0.2549,  ..., -0.2434, -0.2795, -0.2629],\n",
      "          [-0.2164, -0.3245, -0.2638,  ..., -0.2848, -0.3107, -0.2919],\n",
      "          [-0.2082, -0.2572, -0.3018,  ..., -0.2931, -0.2742, -0.3113]],\n",
      "\n",
      "         [[ 0.2001,  0.1251,  0.2131,  ...,  0.2210,  0.1894,  0.2122],\n",
      "          [ 0.1274,  0.1709,  0.1700,  ...,  0.1299,  0.1255,  0.1667],\n",
      "          [ 0.1264,  0.1938,  0.1359,  ...,  0.1744,  0.1976,  0.1967],\n",
      "          ...,\n",
      "          [ 0.1380,  0.1977,  0.1625,  ...,  0.1614,  0.2480,  0.1842],\n",
      "          [ 0.1683,  0.1471,  0.1106,  ...,  0.1797,  0.1481,  0.1364],\n",
      "          [ 0.1402,  0.1382,  0.1876,  ...,  0.1726,  0.1762,  0.1676]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0075,  0.0721,  0.0166,  ...,  0.0449,  0.0216,  0.0019],\n",
      "          [-0.0026,  0.1148,  0.1633,  ...,  0.2500,  0.2111,  0.2095],\n",
      "          [ 0.0250,  0.1111,  0.1872,  ...,  0.1976,  0.1694,  0.1716],\n",
      "          ...,\n",
      "          [ 0.0173,  0.1992,  0.1951,  ...,  0.1671,  0.1649,  0.1789],\n",
      "          [-0.0206,  0.1320,  0.1951,  ...,  0.2026,  0.1541,  0.1776],\n",
      "          [ 0.0404,  0.0339,  0.0450,  ...,  0.0545,  0.0406,  0.1357]],\n",
      "\n",
      "         [[-0.0329,  0.0188,  0.0238,  ..., -0.0589, -0.0921, -0.0178],\n",
      "          [-0.0155,  0.0087, -0.0172,  ...,  0.0016, -0.0203, -0.0421],\n",
      "          [-0.0259,  0.0554,  0.0132,  ...,  0.0963,  0.0620,  0.0071],\n",
      "          ...,\n",
      "          [-0.0374,  0.0287, -0.0100,  ...,  0.0660,  0.0327,  0.0165],\n",
      "          [-0.0502,  0.0298,  0.1475,  ..., -0.0056,  0.0519,  0.1436],\n",
      "          [-0.1698, -0.0261, -0.0217,  ...,  0.0175,  0.0364,  0.0850]],\n",
      "\n",
      "         [[-0.1080,  0.0745, -0.0035,  ...,  0.0551,  0.1183,  0.1022],\n",
      "          [-0.0957, -0.0084,  0.0206,  ...,  0.0397, -0.0279, -0.0539],\n",
      "          [-0.1149, -0.0190,  0.0658,  ..., -0.0306,  0.0883,  0.0370],\n",
      "          ...,\n",
      "          [-0.1062,  0.0524,  0.0215,  ..., -0.0055, -0.0241,  0.0256],\n",
      "          [-0.0954,  0.0234, -0.0651,  ...,  0.0112,  0.0099, -0.0594],\n",
      "          [ 0.0319,  0.0351,  0.0425,  ...,  0.0860,  0.0877, -0.0436]]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "['3', '2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harry\\AppData\\Local\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "fmap_block = dict()\n",
    "grad_block = dict()\n",
    "\n",
    "def backward_hook(module, grad_in, grad_out):\n",
    "    grad_block['grad_in'] = grad_in\n",
    "    grad_block['grad_out'] = grad_out\n",
    "   \n",
    "def forward_hook(module, input, output):\n",
    "    fmap_block['input'] = input\n",
    "    fmap_block['output'] = output\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "label = torch.empty(1, dtype=torch.long).random_(3)\n",
    "input_img = torch.rand(1, 3, 32, 32).requires_grad_()\n",
    "\n",
    "net = Net()\n",
    "\n",
    "# 注册hook->作为函数传入register_forwar_hook\n",
    "net.conv2.register_forward_hook(forward_hook)\n",
    "net.conv2.register_backward_hook(backward_hook)\n",
    "\n",
    "outs = net(input_img)\n",
    "loss = loss_func(outs, label)\n",
    "loss.backward()\n",
    "\n",
    "print(fmap_block['input'], fmap_block['output'])\n",
    "\n",
    "\n",
    "list11 = ['1', '2', '3']\n",
    "\n",
    "import random\n",
    "list2 = random.sample(list11, 2)\n",
    "print(list2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
